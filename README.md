# Dynamic Realtime Animation Control

Our project is targeted at making an application that dynamically detects the user’s expressions and gestures and projects it onto an animation software which then renders a 2D/3D animation realtime that gets broadcasted live. At it’s skeletal state, our project essentially merges facial keypoint detection, keypoint meshing, emotion and gesture tracking with animation. The final rendered animation can be projected / broadcasted onto an application that requires webcam access.

![3 step](https://user-images.githubusercontent.com/64661719/135105688-850bcca1-02cd-4332-9515-7117ebee3133.png)

## MOTIVATION <br>
To look presentable on any video call at any given time.<br>
For protecting yourself and maintaining privacy on the internet.<br>
To make a replacement for video streaming through a webcam that consumes more bandwidth, resulting in using lower bandwidth.<br>

## OBJECTIVES <br>
To make a pose-detection model using OpenCV and Tensor flow.<br>
To also make an emotion detection model.<br>
Animate a render of the newly made model on a live rendering software like Blender or Three.JS!<br>

## OUTPUT <br>

![work](https://user-images.githubusercontent.com/64661719/145720358-6d567064-b2aa-47c4-a70d-d6f832fccbf2.png)
![Finaloutput](https://user-images.githubusercontent.com/64661719/145720325-79fd028d-8f74-476d-b470-88187c4b7c7a.png)

## CONTRIBUTERS <br>
[Harsh-Avinash](https://github.com/Harsh-Avinash)<br>
[Seshank-k](https://github.com/Seshank-k)<br>
[Nishita-Varshney](https://github.com/Nishita-2605)<br>
[Aaryan Bhatiya Ghosh](https://github.com/METALXRAY)

